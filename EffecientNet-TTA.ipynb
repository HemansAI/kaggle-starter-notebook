{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the modules \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision \n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms \n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dir\n",
    "data_dir = \"aerial-cactus-identification/\"\n",
    "train_dir = data_dir + \"train/\"\n",
    "test_dir = data_dir + \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CSV\n",
    "labels = pd.read_csv(\"aerial-cactus-identification/train.csv\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- Fold\n",
    "\n",
    "df = labels.sample(frac=1).reset_index(drop = True)                      # Reset Index\n",
    "df['kfold'] = -1                                                         # Intitialize new col\n",
    "y = labels.has_cactus.values                                             \n",
    "kf = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)        # Intitialize Fold no.\n",
    "idx = kf.get_n_splits(X=df,y=y)                                          # Splitting the dataset\n",
    "print(idx)\n",
    "for fold,(x,y) in enumerate(kf.split(X=df,y=y)):\n",
    "    df.loc[y,'kfold'] = fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "class ImageData(Dataset):\n",
    "    def __init__(self,df,df_target,data_dir,transform):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.df_target = df_target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        print(len(self.df),index)\n",
    "        img_name = self.df[index]\n",
    "        label = self.df_target[index]\n",
    "        \n",
    "        img_path = os.path.join(self.data_dir,img_name)\n",
    "        image = mpimg.imread(img_path)\n",
    "        image = self.transform(image)\n",
    "        return image,label\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "                                transforms.ToPILImage(),\n",
    "                                transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#train_data = ImageData(df = labels,data_dir=train_dir,transform=data_trans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install efficientnet_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_name('efficientnet-b1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model._fc.in_features\n",
    "model._fc = nn.Linear(num_ftrs,1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    batch_t = 2\n",
    "    batch_v = 2\n",
    "    best_score = 0\n",
    "    train_df = df[df.kfold!=fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold==fold].reset_index(drop=True)\n",
    "    \n",
    "    train_im = train_df.id.values.tolist()\n",
    "    train_y = train_df.has_cactus.values\n",
    "    valid_im = valid_df.id.values.tolist()\n",
    "    valid_y = valid_df.has_cactus.values\n",
    "    \n",
    "    \n",
    "    train_data = ImageData(df = train_im,df_target = train_y,data_dir=train_dir,transform=data_trans)\n",
    "    trainloader = DataLoader(dataset = train_data,batch_size = 2)\n",
    "    \n",
    "    valid_data = ImageData(df = valid_im,df_target = valid_y,data_dir=train_dir,transform=data_trans)\n",
    "    validloader = DataLoader(dataset = train_data,batch_size = 2)\n",
    "    \n",
    "    loss_log = []\n",
    "    valid_loss = []\n",
    "    valid_loss = np.Inf\n",
    "\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        batch = 0\n",
    "        for ii ,(data,target) in enumerate(trainloader):\n",
    "            target = target.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            m = nn.Sigmoid()\n",
    "            loss = loss_func(m(output),target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            batch+=1\n",
    "            \n",
    "            if batch%100==0 : \n",
    "                print(\"EPOCH {}  Loss {}  batch  {}\".format(epoch,loss.item(),batch))\n",
    "            if ii%1000 == 0:\n",
    "                loss_log.append(loss.item())\n",
    "        print('Epoch: {} - Loss: {:.6f} v_Loss: {:.6f}'.format(epoch + 1, loss.item(),v_loss.item))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        batch = 0\n",
    "        for ii,(data,target) in enumerate(validloader):\n",
    "            target = target.float()\n",
    "            output = model(data)\n",
    "            \n",
    "            batch+=1\n",
    "            m = nn.Sigmoid()\n",
    "            v_loss = loss_func(m(output),target)\n",
    "            if ii%1000 == 0:\n",
    "                valid_loss.append(v_loss.item())\n",
    "                \n",
    "            if batch%100==0 : \n",
    "                print(\"EPOCH {}  Loss {}  batch  {}\".format(epoch,loss.item(),batch))\n",
    "\n",
    "\n",
    "        print('Epoch: {} - Loss: {:.6f} v_Loss: {:.6f}'.format(epoch + 1, loss.item(),v_loss.item))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(0)\n",
    "train(1)\n",
    "train(2)\n",
    "train(3)\n",
    "train(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TTA - Test Time Augementation\n",
    "\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_data = ImageData(df = submit, data_dir = test_dir, transform = test_transform)\n",
    "test_loader = DataLoader(dataset = test_data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = []\n",
    "model.eval()\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "    output = model(data)    \n",
    "\n",
    "    pred = torch.sigmoid(output)\n",
    "    predicted_vals = pred > 0.5\n",
    "    predict1.append(int(predicted_vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "predict2 = []\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "    output = model(data)    \n",
    "\n",
    "    pred = torch.sigmoid(output)\n",
    "    predicted_vals = pred > 0.5\n",
    "    predict2.append(int(predicted_vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "predict3 = []\n",
    "model.eval()\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "    output = model(data)    \n",
    "\n",
    "    pred = torch.sigmoid(output)\n",
    "    predicted_vals = pred > 0.5\n",
    "    predict3.append(int(predicted_vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "predict4 = []\n",
    "model.eval()\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "    output = model(data)    \n",
    "\n",
    "    pred = torch.sigmoid(output)\n",
    "    predicted_vals = pred > 0.5\n",
    "    predict4.append(int(predicted_vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "predict = (predict1 + predict2 + predict3 + predict4) / 4.0\n",
    "    \n",
    "\n",
    "submit['has_cactus'] = predict\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
